{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = pd.read_csv('data/account.csv', encoding='ISO-8859-1')\n",
    "concerts_1415=pd.read_csv('data/concerts_2014-15.csv')\n",
    "concerts=pd.read_csv('data/concerts.csv')\n",
    "sample_submission=pd.read_csv('data/sample_submission.csv')\n",
    "subscriptions=pd.read_csv('data/subscriptions.csv')\n",
    "test=pd.read_csv('data/test.csv')\n",
    "tickets_all=pd.read_csv('data/tickets_all.csv')\n",
    "train=pd.read_csv('data/train.csv')\n",
    "zipcodes=pd.read_csv('data/zipcodes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Subscription and Construct Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle subscription data\n",
    "def handle_subscription(group):\n",
    "    # Safely handle NaN values in cities\n",
    "    shipping_city = group['shipping.city'].fillna(\"\")\n",
    "    billing_city = group['billing.city'].fillna(\"\")\n",
    "\n",
    "    # If there are any subscription data\n",
    "    if group['season'].notna().any():\n",
    "        return pd.DataFrame({\n",
    "\n",
    "            # Total season:\n",
    "            'total_season': [group.shape[0]],\n",
    "\n",
    "            # Package features\n",
    "            'full_package': [(group['package'] == 'Full').sum()],\n",
    "            'quartet_package': [(group['package'].isin(['Quartet', 'Quartet A', 'Quartet B', 'Quartet CC'])).sum()],\n",
    "            'trio_package': [(group['package'].isin(['Trio', 'Trio A', 'Trio B'])).sum()],\n",
    "            'cyo_package': [(group['package'] == 'CYO').sum()],\n",
    "            'full_upgrade_package': [(group['package'] == 'Full upgrade').sum()],\n",
    "\n",
    "            # Seat features\n",
    "            'total_seats_sub': [group['no.seats'].sum()],\n",
    "\n",
    "            # Location features\n",
    "            'location_num_sub': [group['location'].nunique()],\n",
    "            # 'location_near_resident_num_sub': [(group['location'] == shipping_city).sum() if shipping_city.any() else (group['location'] == billing_city).sum()],\n",
    "            # 'location_not_resident_num_sub': [group.shape[0] - (group['location'] == shipping_city).sum() if shipping_city.any() else group.shape[0] - (group['location'] == billing_city).sum()],\n",
    "\n",
    "            # Section features\n",
    "            'section_type_numbers': [group['section'].nunique()],\n",
    "            'premium_orchestra_number': [(group['section'] == 'Premium Orchestra').sum()],\n",
    "            'orchestra_number': [(group['section'].isin(['Orchestra', 'Orchestra Front', 'Orchestra Rear'])).sum()],\n",
    "            'balcony_number': [(group['section'].isin(['Balcony Front', 'Balcony Rear', 'Balcony', 'Santa Rosa'])).sum()],\n",
    "            'dress_circle_number': [(group['section'] == 'Dress Circle').sum()],\n",
    "            'Gallery_number': [(group['section'] == 'Gallery').sum()],\n",
    "            'Box_number': [(group['section'].isin(['Box', 'Box House Left', 'Box House Right'])).sum()],\n",
    "            'floor_number': [(group['section'] == 'Floor').sum()],\n",
    "\n",
    "            # Price features (some seasons do not have price level data using another feature to present it and calculate mean without them)\n",
    "            'mean_price_level': [-1 if (~group['price.level'].isna()).sum()==0 else group[~group['price.level'].isna()]['price.level'].mean()],\n",
    "\n",
    "            'non_price_level_subscription': [group['season'].isin(['2002-2003','2003-2004','2004-2005']).sum()],\n",
    "\n",
    "            # Subscription tier features\n",
    "            'mean_subscription_tier': [group['subscription_tier'].mean()],\n",
    "            'multiple_subs_number': [(group['multiple.subs'] == 'yes').sum()],\n",
    "\n",
    "            # other features\n",
    "            'account.id': [group['account.id'].iloc[0]],\n",
    "            'label': [group['label'].iloc[0]] if 'label' in group.columns else [-1],\n",
    "            'shipping.zip.code': [group['shipping.zip.code'].iloc[0]],\n",
    "            'billing.zip.code': [group['billing.zip.code'].iloc[0]],\n",
    "            'shipping_city': [shipping_city.iloc[0]],\n",
    "            'billing_city': [billing_city.iloc[0]],\n",
    "            'relationship': [group['relationship'].iloc[0]],\n",
    "            'amount.donated.2013': [group['amount.donated.2013'].iloc[0]],\n",
    "            'amount.donated.lifetime': [group['amount.donated.lifetime'].iloc[0]],\n",
    "            'no.donations.lifetime': [group['no.donations.lifetime'].iloc[0]],\n",
    "            'first.donated': [group['first.donated'].iloc[0]],\n",
    "        })\n",
    "    \n",
    "    # no subscription data, just fill with -1\n",
    "    else:\n",
    "        return pd.DataFrame({\n",
    "            # Total season:\n",
    "            'total_season': [-1],\n",
    "\n",
    "            # Package features\n",
    "            'full_package': [-1],\n",
    "            'quartet_package': [-1],\n",
    "            'trio_package': [-1],\n",
    "            'cyo_package': [-1],\n",
    "            'full_upgrade_package': [-1],\n",
    "\n",
    "            # Seat features\n",
    "            'total_seats_sub': [-1],\n",
    "\n",
    "            # Location features\n",
    "            'location_num_sub': [-1],\n",
    "            # 'location_near_resident_num_sub': [-1],\n",
    "            # 'location_not_resident_num_sub': [-1],\n",
    "\n",
    "            # Section features\n",
    "            'section_type_numbers': [-1],\n",
    "            'premium_orchestra_number': [-1],\n",
    "            'orchestra_number': [-1],\n",
    "            'balcony_number': [-1],\n",
    "            'dress_circle_number': [-1],\n",
    "            'Gallery_number': [-1],\n",
    "            'Box_number': [-1],\n",
    "            'floor_number': [-1],\n",
    "\n",
    "            # Price features\n",
    "            'mean_price_level': [-1],\n",
    "\n",
    "            'non_price_level_subscription': [-1],\n",
    "\n",
    "            # Subscription tier features\n",
    "            'mean_subscription_tier': [-1],\n",
    "            'multiple_subs_number': [-1],\n",
    "\n",
    "            # other features\n",
    "            'account.id': [group['account.id'].iloc[0]],\n",
    "            \n",
    "            'label': [group['label'].iloc[0]] if 'label' in group.columns else [-1],\n",
    "            \n",
    "            'shipping.zip.code': [group['shipping.zip.code'].iloc[0]],\n",
    "            'billing.zip.code': [group['billing.zip.code'].iloc[0]],\n",
    "            'shipping_city': [shipping_city.iloc[0]],\n",
    "            'billing_city': [billing_city.iloc[0]],\n",
    "            'relationship': [group['relationship'].iloc[0]],\n",
    "            'amount.donated.2013': [group['amount.donated.2013'].iloc[0]],\n",
    "            'amount.donated.lifetime': [group['amount.donated.lifetime'].iloc[0]],\n",
    "            'no.donations.lifetime': [group['no.donations.lifetime'].iloc[0]],\n",
    "            'first.donated': [group['first.donated'].iloc[0]],\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24549\\AppData\\Local\\Temp\\ipykernel_91216\\569496751.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_merged = train_merged.groupby('account.id',group_keys=False).apply(handle_subscription)\n"
     ]
    }
   ],
   "source": [
    "# Merge train with account\n",
    "train_merged = pd.merge(train, account, on='account.id', how='left')\n",
    "\n",
    "# Merge train with subscriptions\n",
    "train_merged = pd.merge(train_merged, subscriptions, on='account.id', how='left')\n",
    "\n",
    "# Apply the function using groupby\n",
    "train_merged = train_merged.groupby('account.id',group_keys=False).apply(handle_subscription)\n",
    "\n",
    "train_merged=train_merged.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Tickets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4' '1' '3' '2' nan '0' 'Adult' 'Youth' 'GA' '4.0']\n"
     ]
    }
   ],
   "source": [
    "tickets_all['multiple.tickets'].isna().sum()\n",
    "\n",
    "print(tickets_all['price.level'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The player in season 2014-2015 \n",
    "player_1415=['Nicholas McGegan','Steven Isserlis','Julian Wachner','Andreas Scholl','Dominique Labelle','Christopher Ainslie','Thomas Cooley','Dashon Burton',\\\n",
    "             'Bruce Lamott','Sherezade Panthaki','Clifton Massey','Brian Thorsett','Jeffrey Fields',\\\n",
    "                'Rachel Podger',' Ted Huffman']\n",
    "\n",
    "key_content_1415 = ['LÕestro armonico','VIVALDI','HAYDN','HANDEL','BACH','CANTATA','TELEMANN','CPE BACH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tickets(group):\n",
    "    \n",
    "    other_columns = group.columns.difference([\n",
    "        'no.seats', 'price.level', 'location', 'set', 'multiple.tickets', 'season','marketing.source'\n",
    "    ])\n",
    "    if group['season'].notna().any():\n",
    "\n",
    "        aggregated_data=pd.DataFrame({\n",
    "\n",
    "            # tickets sum\n",
    "            'sum_tickets': [group['no.seats'].sum()],\n",
    "\n",
    "            # price features\n",
    "            'average_price_level': [group['price.level'].mean()],\n",
    "            \n",
    "            # seats number features\n",
    "\n",
    "            'total_seats_ticket': [group['no.seats'].sum()],\n",
    "\n",
    "            # localtion features\n",
    "            'location_num_ticket': [group['location'].nunique()],\n",
    "            # 'location_near_resident_num_ticket': [(group['location'] == group['shipping_city']).sum() if group['shipping_city'].notna().any() else (group['location'] == group['billing_city']).sum()],\n",
    "            # 'location_not_resident_num_ticket': [group.shape[0] - (group['location'] == group['shipping_city']).sum() if group['shipping_city'].notna().any() else group.shape[0] - (group['location'] == group['billing_city']).sum()],\n",
    "            \n",
    "            # set features\n",
    "\n",
    "            'set sum': [group['set'].sum()],\n",
    "\n",
    "            # multiple_tickets features\n",
    "            'multiple_tickets_num': [(group['multiple.tickets'] == 'yes').sum()],\n",
    "\n",
    "\n",
    "            \n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        aggregated_data=pd.DataFrame({\n",
    "            # tickets sum\n",
    "            'sum_tickets': [-1],\n",
    "\n",
    "            # price features\n",
    "            'average_price_level': [-1],\n",
    "            \n",
    "            # seats number features\n",
    "\n",
    "            'total_seats_ticket': [-1],\n",
    "\n",
    "            # localtion features\n",
    "            'location_num_ticket': [-1],\n",
    "            # 'location_near_resident_num_ticket': [-1],\n",
    "            # 'location_not_resident_num_ticket': [-1],\n",
    "            \n",
    "            # set features\n",
    "\n",
    "            'set sum': [-1],\n",
    "\n",
    "            # multiple_tickets features\n",
    "            'multiple_tickets_num': [-1],\n",
    "\n",
    "        })\n",
    "\n",
    "    other_features = group.iloc[0][other_columns].to_frame().T.reset_index(drop=True)\n",
    "\n",
    "    # Set each column in `other_features` to its original data type\n",
    "    for col in other_columns:\n",
    "        other_features[col] = other_features[col].astype(group[col].dtype)\n",
    "\n",
    "    for season, location in zip(group['season'],group['location']):\n",
    "        print(season,location)\n",
    "\n",
    "\n",
    "    final_result = pd.concat([aggregated_data, other_features], axis=1)\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate the mean for the set column, first fillna with the mean of the value\n",
    "tickets_all['set']=tickets_all.groupby('account.id')['set'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "def handle_price_level(x):\n",
    "    if x in [\"Adult\", \"Youth\", \"GA\"]:\n",
    "        return float('nan')  # Set these values to NaN\n",
    "    return float(x)  # Convert other values to float\n",
    "\n",
    "# Apply the function to replace \"Adult\", \"Youth\", and \"GA\" with NaN, preparing for mean replacement\n",
    "tickets_all['price.level'] = tickets_all['price.level'].apply(handle_price_level)\n",
    "\n",
    "# Fill NaN values in 'price.level' with the group mean based on 'account.id'\n",
    "tickets_all['price.level'] = tickets_all.groupby('account.id')['price.level'].transform(lambda x: x.fillna(x.mean()) if x.notna().any() else x.fillna(-1))\n",
    "\n",
    "# merge train with tickets all\n",
    "\n",
    "train_merged = pd.merge(train_merged,tickets_all,on='account.id',how='left')\n",
    "\n",
    "train_merged = train_merged.groupby('account.id',group_keys=False).apply(handle_tickets)\n",
    "\n",
    "train_merged=train_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values,Duplicates and Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged[train_merged['mean_price_level'].isna()]['account.id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Duplicate Rows\n",
    "train_merged=train_merged.drop_duplicates()\n",
    "\n",
    "# Delete shipping_city and billing_city because shipping_city is the same as shipping zip code and billing_city is the same as billing zip code\n",
    "\n",
    "train_merged=train_merged.drop(['shipping_city','billing_city'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA with empty string\n",
    "train_merged['shipping.zip.code']=train_merged['shipping.zip.code'].fillna(\"\")\n",
    "train_merged['billing.zip.code']=train_merged['billing.zip.code'].fillna(\"\")\n",
    "train_merged['relationship']=train_merged['relationship'].fillna(\"\")\n",
    "train_merged['first.donated']=train_merged.apply(lambda x: '1800-01-01 00:00:00' if x['no.donations.lifetime']==0 else (0 if pd.isna(x['first.donated']) else x['first.donated']),axis=1)\n",
    "\n",
    "train_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn first donated to timestamp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged['first.donated'] = pd.to_datetime(train_merged['first.donated'])\n",
    "\n",
    "train_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "train_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## WOE Encoding\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Use WoE to encode shipping zip code and billing zip code\n",
    "\n",
    "woe = ce.WOEEncoder([ 'shipping.zip.code', 'billing.zip.code','relationship'])\n",
    "\n",
    "train_merged['shipping.zip.code'] = train_merged['shipping.zip.code'].astype(str)\n",
    "train_merged['billing.zip.code'] = train_merged['billing.zip.code'].astype(str)\n",
    "\n",
    "woe.fit(train_merged[['shipping.zip.code', 'billing.zip.code','relationship']], train_merged['label'])\n",
    "\n",
    "train_merged[['shipping.zip.code', 'billing.zip.code','relationship']] = woe.transform(train_merged[['shipping.zip.code', 'billing.zip.code','relationship']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days since the first donation\n",
    "\n",
    "now = pd.to_datetime('2014-09-01')\n",
    "\n",
    "train_merged['days_since_first_donation'] = (now - train_merged['first.donated']).dt.days\n",
    "\n",
    "# drop first donated column\n",
    "\n",
    "train_merged=train_merged.drop(['first.donated'],axis=1)\n",
    "\n",
    "train_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name\n",
    "\n",
    "test['account.id']=test['ID']\n",
    "test.drop(['ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test with account\n",
    "test_merged = pd.merge(test, account, on='account.id', how='left')\n",
    "\n",
    "# Merge test with subscriptions\n",
    "test_merged = pd.merge(test_merged, subscriptions, on='account.id', how='left')\n",
    "\n",
    "# Apply the function using groupby\n",
    "test_merged = test_merged.groupby('account.id',group_keys=False).apply(handle_subscription)\n",
    "\n",
    "test_merged=test_merged.reset_index(drop=True)\n",
    "\n",
    "test_merged.drop(['label'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test with tickets all\n",
    "\n",
    "test_merged = pd.merge(test_merged,tickets_all,on='account.id',how='left')\n",
    "\n",
    "test_merged = test_merged.groupby('account.id',group_keys=False).apply(handle_tickets)\n",
    "\n",
    "test_merged=test_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Duplicate Rows\n",
    "test_merged=test_merged.drop_duplicates()\n",
    "\n",
    "# Delete shipping_city and billing_city because shipping_city is the same as shipping zip code and billing_city is the same as billing zip code\n",
    "\n",
    "test_merged=test_merged.drop(['shipping_city','billing_city'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA with empty string\n",
    "test_merged['shipping.zip.code']=test_merged['shipping.zip.code'].fillna(\"\")\n",
    "test_merged['billing.zip.code']=test_merged['billing.zip.code'].fillna(\"\")\n",
    "test_merged['relationship']=test_merged['relationship'].fillna(\"\")\n",
    "test_merged['first.donated']=test_merged.apply(lambda x: '1800-01-01 00:00:00' if x['no.donations.lifetime']==0 else (0 if pd.isna(x['first.donated']) else x['first.donated']),axis=1)\n",
    "\n",
    "# change to the type of datetime\n",
    "test_merged['first.donated'] = pd.to_datetime(test_merged['first.donated'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "\n",
    "test_merged[['shipping.zip.code', 'billing.zip.code','relationship']] = woe.transform(test_merged[['shipping.zip.code', 'billing.zip.code','relationship']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days since the first donation\n",
    "\n",
    "now = pd.to_datetime('2014-09-01')\n",
    "\n",
    "test_merged['days_since_first_donation'] = (now - test_merged['first.donated']).dt.days\n",
    "\n",
    "# drop first donated column\n",
    "\n",
    "test_merged=test_merged.drop(['first.donated'],axis=1)\n",
    "\n",
    "test_merged.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['account.id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "\n",
    "X_train=train_merged.drop(['label','account.id'],axis=1)\n",
    "y_train=train_merged['label']\n",
    "\n",
    "X_test=test_merged.drop(['account.id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "\n",
    "# Scale train data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "train_scaled=X_train.copy()\n",
    "\n",
    "scaler.fit(train_scaled[numerical_cols])\n",
    "\n",
    "train_scaled[numerical_cols] = scaler.transform(train_scaled[numerical_cols])\n",
    "\n",
    "# Scale test data\n",
    "\n",
    "test_scaled=X_test.copy()\n",
    "\n",
    "test_scaled[numerical_cols] = scaler.transform(test_scaled[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor as cat\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold cross validation\n",
    "\n",
    "splits = 5\n",
    "\n",
    "kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "def cv_model(clf, x_train, y_train, x_test, clf_name , kf):\n",
    "    \n",
    "    cv_scores = []\n",
    "\n",
    "    test_all = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(x_train, y_train)):\n",
    "\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "\n",
    "        trn_x, trn_y, val_x, val_y = x_train.iloc[train_index], y_train[train_index], x_train.iloc[valid_index], y_train[valid_index]\n",
    "\n",
    "        # LightGBM\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            # training parameters\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 4,\n",
    "                'num_leaves': 2 ** 4,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_fraction': 0.7,\n",
    "                'bagging_freq': 10,\n",
    "                'learning_rate': 0.15,\n",
    "                'seed': 2022,\n",
    "                'n_jobs':-1,\n",
    "                'verbose':-1\n",
    "            }\n",
    "            # model training\n",
    "            model = clf.train(params, train_matrix, 30000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[])\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "        \n",
    "        # XGBoost\n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(x_test)\n",
    "            \n",
    "            # training parameters\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 7,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.125,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': 36,\n",
    "                      \"silent\": True,\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "\n",
    "            # model training\n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist,verbose_eval=True)\n",
    "            val_pred  = model.predict(valid_matrix)\n",
    "            test_pred = model.predict(test_matrix)\n",
    "\n",
    "        # Catboost         \n",
    "        if clf_name == \"cat\":\n",
    "            \n",
    "            # training parameters\n",
    "            params = {'learning_rate': 0.134, \n",
    "            'depth': 10 ,\n",
    "            'l2_leaf_reg': 5, \n",
    "            'bootstrap_type': 'Bernoulli',\n",
    "            'od_type': 'Iter', \n",
    "            'od_wait': 2000, \n",
    "            'random_seed': 164, \n",
    "            'allow_writing_files': False\n",
    "            }\n",
    "\n",
    "            # model training\n",
    "            model = clf(iterations=30000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      cat_features=[], use_best_model=True, verbose=600)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(x_test)\n",
    "        \n",
    "\n",
    "        test_all.append(test_pred)\n",
    "\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        \n",
    "        print(cv_scores)\n",
    "\n",
    "    # output\n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "\n",
    "    # Convert test_all (list of lists) to a NumPy array\n",
    "    test_all_array = np.array(test_all)\n",
    "\n",
    "    # Calculate the mean across the lists (axis=0 computes mean element-wise across all lists)\n",
    "    mean_output = np.mean(test_all_array, axis=0)\n",
    "\n",
    "    return mean_output\n",
    "        \n",
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\", kf)\n",
    "    return lgb_test\n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\", kf)\n",
    "    return xgb_test\n",
    "\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_test = cv_model(cat, x_train, y_train, x_test, \"cat\", kf)\n",
    "    return cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_test=lgb_model(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_test=xgb_model(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test=cat_model(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Data to Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the results to the test merged\n",
    "\n",
    "# I choose catboost for the final model\n",
    "\n",
    "output=pd.DataFrame({'ID':test_merged['account.id'],'Predicted':cat_test})\n",
    "\n",
    "output.to_csv('submission.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
